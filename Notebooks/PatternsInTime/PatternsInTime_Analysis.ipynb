{"cells":[{"source":"# Patterns in Time\n\n## 1. Background\nThe objective of this collection of Jupyter notebooks is to establish a systematic process for identifying time patterns within a user's data. These time patterns help in recognizing how external factors, such as the calendar, influence our users.\n\nWe aim to uncover recurring patterns across various aspects of time, such as: \n- Hours of the day\n- Days of the week\n- Days of the month\n- Parts of the month\n- Months\n\nExample questions that users could ask or receive answers to include:\n\n- Is there a tendency for headaches to occur mostly on Mondays?\n- Do I generally experience higher levels of happiness later in the month?\n- Is there a noticeable difference in my activity levels on certain days?\n- How does my blood sugar level fluctuate throughout the day?\n\nThese notebooks were developed as a preliminary step towards an analytical extension operating on LLIF (Live Learn Innovate Foundation) and utilized by BestLife.\n\n## 2. Outlier Detection Methods\n\nInitially, we considered correlation methods to identify patterns within the data. However, after initial testing, we found these methods to be less than ideal. Following extensive discussions, we have decided to adopt generic statistical methods to determine the presence of outliers in a given data series, which we believe to be a more suitable metric. In the following section, I will outline some of the methods that we have chosen to explore.\n\n\n### Standard Deviation on a Normalized Data Series\n\nAs an alternative to correlation coefficients, we explored the concept of using standard deviation on a normalized data series to assess the likelihood of outliers. Our objective was to derive a score ranging from 0 to 1, considering both the presence and magnitude of outliers in the data. Despite extensive exploration, we were unable to identify an approach that met all of our requirements.\n\n### Standard Deviation in Combination with the Mean of the Data Series\n\nBuilding upon the concept of using standard deviation, we abandoned the normalization process and instead investigated methods involving rules based on both the standard deviation and mean of the data series. While this approach demonstrated effectiveness with many basic testing datasets, we harbored reservations about its suitability for the final product. Nonetheless, it presents an opportunity to enhance the initial solution by incorporating mechanisms to determine the magnitude of detected outliers.\n\n### Z-score outlier detection [SOURCE](https://www.machinelearningplus.com/machine-learning/how-to-detect-outliers-with-z-score/)\nWe investigated the possibility of using the Z-score method for outlier detection, which has provided us with satisfactory results for the majority of our testing datasets. This method stands as a viable option for our purposes. Additionally, we experimented with developing a process to generate a score ranging from 0 to 1, but unfortunately, this approach did not yield favorable results.\n\n### Interquartile Outlier detection [SOURCE](https://online.stat.psu.edu/stat200/lesson/3/3.2)\nThis method utilizes the quartile properties of the data series along with the distance from the first quartile (Q1) to the third quartile (Q3) to identify potential outliers within the data. Our experimentation revealed that this approach consistently produced the most favorable results, leading us to select it for implementation. With this method, we can obtain the outliers within the data series along with their corresponding calendar representations.\n\nAdditionally, we attempted to generate a score ranging from 0 to 1 to summarize the outlier detection process. However, this approach did not yield satisfactory results.\n\n### Bayesian Statistics Approach\nUtilizing a Bayesian statistics approach can offer insights into whether the distribution of the data series conforms to a normal distribution, which is expected in the absence of temporal patterns. Here are the steps involved:\n\n1. **Choose a Bayesian Model**: Select an appropriate Bayesian model that represents the underlying distribution of the data. In this case, a normal distribution would be suitable.\n2. **Define Priors**: Specify prior distributions for the parameters of the chosen model. Priors reflect beliefs about the parameters before observing the data. They can be informed by domain knowledge or chosen to be uninformative if prior knowledge is limited.\n3. **Compute Posteriors**: Update the priors using Bayes' theorem to calculate the posterior distributions of the parameters given the observed data. This typically involves techniques such as Markov Chain Monte Carlo (MCMC) or variational inference.\n4. **Identify Outliers**: With the posterior distributions of the parameters, outliers can be identified as data points with low probability under the fitted model. Outliers can be defined using thresholds based on credible intervals or by assessing the probability of a data point given the model.\n5. **Evaluate Model Fit**: Assess the overall fit of the Bayesian model to the data and validate whether the detected outliers are genuine anomalies or errors in the data.\n\nWhile the Bayesian approach offers a rigorous method for outlier detection, the Interquartile Range (IQR) method is simpler and should serve our purposes nearly as effectively. Its ease of implementation and robustness make it a suitable choice for our problem.\n\n## 3. Included Files\n\n1. PatternsInTime_Analysis.ipynb: This is the main file for the LLIFE extension. It explores the generated domain-like testing datasets created via the PatternsInTime_GenerateTestingSets file. It serves as the primary building block for deploying to the LLIF codebase.\n2. PatternsInTime_OutlierDetectionTesting.ipynb: This notebook was primarily used for researching different outlier detection methods. It contains functions used for outlier detection and tests the chosen method on given testing datasets.\n3. PatternsInTime_GenerateTestingSets.ipynb: This file is solely used to generate the domain-like datasets required for the main analysis in PatternsInTime_Analysis.\n\n\n## 4. Not Included\n\n- **Time Between Events** - This has been a concept that we also want to explore, but we have yet to come to come to decisive conslusion on how to exactly use this.","metadata":{},"cell_type":"markdown","id":"46cc9a1f-579a-4537-aa9e-bd0a6d0a1c4b"},{"source":"import warnings\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nfrom PatternsInTime_HelperFunctions import get_documents_data_into_df, load_file_as_list_of_documents, add_time_columns_to_dataframe, detect_outliers_iqr\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1712652177918,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import warnings\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nfrom PatternsInTime_HelperFunctions import get_documents_data_into_df, load_file_as_list_of_documents, add_time_columns_to_dataframe, detect_outliers_iqr\n\nwarnings.filterwarnings(\"ignore\")","lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec"},"cell_type":"code","id":"7115a703-f7ea-4f88-91b7-c1d8baa679b6","outputs":[],"execution_count":89},{"source":"# SETUP MAPPINGS\n\ngroup_by_categories = [\"Hours\", \"Week_Days\", \"Days_Of_Month\", \"Month_Parts\", \"Months\"]\n\ndays_in_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nmonths_in_year = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\nhours_in_day = [\"07:00\", \"08:00\", \"09:00\", \"10:00\",\n                \"11:00\", \"12:00\", \"13:00\", \"14:00\",\n                \"15:00\", \"16:00\", \"17:00\", \"18:00\",\n                \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\"]\n                # \"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\",\n                # \"05:00\", \"06:00\", Removed as those are usually not active hours\n                # Don't want to pollute the calculation\nparts_of_month = [\"1/5\", \"2/5\", \"3/5\", \"4/5\", \"5/5\"]\ndays_in_month = [i+1 for i in range(31)]\n\nCategoryToExpectedRowValuesMapping = {\"Hours\": hours_in_day,\n                                      \"Week_Days\": days_in_week,\n                                      \"Days_Of_Month\":days_in_month,\n                                      \"Month_Parts\": parts_of_month,\n                                      \"Months\": months_in_year}\n\n# Create a dictionary to map hour integers to hour strings\nhour_mapping = {i: hour for i, hour in enumerate(hours_in_day)}","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1712652178286,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# SETUP MAPPINGS\n\ngroup_by_categories = [\"Hours\", \"Week_Days\", \"Days_Of_Month\", \"Month_Parts\", \"Months\"]\n\ndays_in_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nmonths_in_year = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\nhours_in_day = [\"07:00\", \"08:00\", \"09:00\", \"10:00\",\n                \"11:00\", \"12:00\", \"13:00\", \"14:00\",\n                \"15:00\", \"16:00\", \"17:00\", \"18:00\",\n                \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\"]\n                # \"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\",\n                # \"05:00\", \"06:00\", Removed as those are usually not active hours\n                # Don't want to pollute the calculation\nparts_of_month = [\"1/5\", \"2/5\", \"3/5\", \"4/5\", \"5/5\"]\ndays_in_month = [i+1 for i in range(31)]\n\nCategoryToExpectedRowValuesMapping = {\"Hours\": hours_in_day,\n                                      \"Week_Days\": days_in_week,\n                                      \"Days_Of_Month\":days_in_month,\n                                      \"Month_Parts\": parts_of_month,\n                                      \"Months\": months_in_year}\n\n# Create a dictionary to map hour integers to hour strings\nhour_mapping = {i: hour for i, hour in enumerate(hours_in_day)}","lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec"},"cell_type":"code","id":"607a1c5f-e165-4151-803e-aab66b4b209a","outputs":[],"execution_count":90},{"source":"# HELPER FUNCTIONS\n\ndef load_file_as_list_of_documents(filepath):\n    \"\"\"\n    Load a JSON file containing a list of documents and return them as a list.\n\n    Args:\n    filepath (str): The path to the JSON file.\n\n    Returns:\n    list: A list containing the documents loaded from the JSON file.\n    \"\"\"\n    f = open(filepath)\n    list_of_documents = []\n    documents = json.load(f)\n    for document in documents:\n        list_of_documents.append(document)\n\n    return list_of_documents\n\n\ndef get_documents_data_into_df(\n    documents: list,\n    fields: list,\n) -> pd.DataFrame:\n    \"\"\"\n    Filters through nested fields in JSON-like objects and converts them to a Pandas DataFrame with desired fields as columns.\n\n    Args:\n    documents (list): A list of dictionaries representing documents with nested fields.\n    fields (list): A list of strings representing the fields to extract from the documents.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the specified fields as columns, with data extracted from the documents.\n\n    Example:\n    documents = [\n        {\"name\": {\"first\": \"John\", \"last\": \"Doe\"}, \"age\": 30},\n        {\"name\": {\"first\": \"Jane\", \"last\": \"Smith\"}, \"age\": 25}\n    ]\n    fields = [\"name.first\", \"name.last\", \"age\"]\n    df = get_documents_data_into_df(documents, fields)\n    print(df)\n    Output:\n       first    last   age\n    0   John     Doe  30.0\n    1   Jane   Smith  25.0\n    \"\"\"\n    def get_field_data(input_doc, field):\n        \"\"\"\n        Extracts data from nested fields in a document.\n\n        Args:\n        input_doc (dict): The document from which to extract data.\n        field (str): The field to extract, possibly nested.\n\n        Returns:\n        obj: The data corresponding to the specified field.\n        \"\"\"\n        for subfield in field.split(\".\"):\n            if isinstance(input_doc, list):\n                input_doc = input_doc[0]\n            input_doc = input_doc.get(subfield)\n        return input_doc\n\n    column_names = [field.split(\".\")[-1] for field in fields]\n    return_df = pd.DataFrame(columns=column_names)\n\n    for document in documents:\n        temp_df = pd.DataFrame(\n            {column: get_field_data(document, field) for column, field in zip(column_names, fields)},\n            index=range(1)\n        )\n        return_df = pd.concat([return_df, temp_df]).reset_index(drop=True)\n\n    return return_df.fillna(value=np.nan)\n\ndef add_time_columns_to_dataframe(df):\n    \"\"\"\n    Adds additional time-related columns to a DataFrame based on the 'timestamp' column.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing a 'timestamp' column.\n\n    Returns:\n    pd.DataFrame: The DataFrame with added time-related columns.\n\n    Example:\n    import pandas as pd\n    df = pd.DataFrame({\n        'timestamp': pd.date_range(start='2022-01-01', end='2022-01-05', freq='D')\n    })\n    df = add_time_columns_to_dataframe(df)\n    print(df)\n    Output:\n       timestamp  Hours   Months  Days_Of_Month  Week_Days Month_Parts\n    0 2022-01-01      0  January              1   Saturday         1/5\n    1 2022-01-02      0  January              2     Sunday         1/5\n    2 2022-01-03      0  January              3     Monday         1/5\n    3 2022-01-04      0  January              4    Tuesday         2/5\n    4 2022-01-05      0  January              5  Wednesday         2/5\n    \"\"\"\n    # Add Hours column\n    df[\"Hours\"] = df[\"timestamp\"].dt.hour\n\n    # Add Months column\n    df[\"Months\"] = df[\"timestamp\"].dt.month_name()\n\n    # Add Days_Of_Month column\n    df[\"Days_Of_Month\"] = df[\"timestamp\"].dt.day\n\n    # Add Week_Days column\n    df[\"Week_Days\"] = df[\"timestamp\"].dt.day_name()\n\n    # Add Month_Parts column\n    df[\"Month_Parts\"] = df[\"Days_Of_Month\"] / 6\n    # Map Month_Parts to respective values\n    conditions = [\n        (df[\"Month_Parts\"] <= 1),\n        (df[\"Month_Parts\"] > 1) & (df[\"Month_Parts\"] <= 2),\n        (df[\"Month_Parts\"] > 2) & (df[\"Month_Parts\"] <= 3),\n        (df[\"Month_Parts\"] > 3) & (df[\"Month_Parts\"] <= 4),\n        (df[\"Month_Parts\"] > 4)\n    ]\n    values = [\"1/5\", \"2/5\", \"3/5\", \"4/5\", \"5/5\"]\n    df[\"Month_Parts\"] = np.select(conditions, values)\n    \n    return df\n\ndef add_expected_rows_to_count_analysis(df, category):\n    \"\"\"\n    Adds expected rows with zero count to a DataFrame based on a specified category.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing count analysis.\n    category (str): The category column to check and add missing rows.\n\n    Returns:\n    pd.DataFrame: The DataFrame with added expected rows.\n    \"\"\"\n    expected_row_values = CategoryToExpectedRowValuesMapping.get(category)\n    missing_rows = []\n    for value in expected_row_values:\n        if value not in df[category].values:\n            missing_rows.append(pd.DataFrame({category: [value], \"Count\": [0]}))\n    \n    if missing_rows:\n        df = pd.concat([df] + missing_rows, ignore_index=True)\n\n    # Sort the DataFrame based on the category to maintain order\n    df[category] = pd.Categorical(df[category], categories=expected_row_values, ordered=True)\n    df = df.sort_values(category).reset_index(drop=True)\n    \n    return df\n\n\ndef detect_outliers_iqr(data, k=1.5):\n    \"\"\"\n    Detect outliers in the data using IQR (Interquartile Range) method.\n\n    Parameters:\n    data (numpy.ndarray): Input data as a numpy array.\n    k (float): Coefficient to scale the IQR. Data points beyond k times the IQR\n               from the first and third quartiles are considered outliers. Default is 1.5.\n\n    Returns:\n    numpy.ndarray: Boolean array indicating whether each data point is an outlier or not.\n    \"\"\"\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n    lower_bound = q1 - k * iqr\n    upper_bound = q3 + k * iqr\n    return (data < lower_bound) | (data > upper_bound)","metadata":{"executionCancelledAt":null,"executionTime":20,"lastExecutedAt":1712652900759,"lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# HELPER FUNCTIONS\n\ndef load_file_as_list_of_documents(filepath):\n    \"\"\"\n    Load a JSON file containing a list of documents and return them as a list.\n\n    Args:\n    filepath (str): The path to the JSON file.\n\n    Returns:\n    list: A list containing the documents loaded from the JSON file.\n    \"\"\"\n    f = open(filepath)\n    list_of_documents = []\n    documents = json.load(f)\n    for document in documents:\n        list_of_documents.append(document)\n\n    return list_of_documents\n\n\ndef get_documents_data_into_df(\n    documents: list,\n    fields: list,\n) -> pd.DataFrame:\n    \"\"\"\n    Filters through nested fields in JSON-like objects and converts them to a Pandas DataFrame with desired fields as columns.\n\n    Args:\n    documents (list): A list of dictionaries representing documents with nested fields.\n    fields (list): A list of strings representing the fields to extract from the documents.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the specified fields as columns, with data extracted from the documents.\n\n    Example:\n    documents = [\n        {\"name\": {\"first\": \"John\", \"last\": \"Doe\"}, \"age\": 30},\n        {\"name\": {\"first\": \"Jane\", \"last\": \"Smith\"}, \"age\": 25}\n    ]\n    fields = [\"name.first\", \"name.last\", \"age\"]\n    df = get_documents_data_into_df(documents, fields)\n    print(df)\n    Output:\n       first    last   age\n    0   John     Doe  30.0\n    1   Jane   Smith  25.0\n    \"\"\"\n    def get_field_data(input_doc, field):\n        \"\"\"\n        Extracts data from nested fields in a document.\n\n        Args:\n        input_doc (dict): The document from which to extract data.\n        field (str): The field to extract, possibly nested.\n\n        Returns:\n        obj: The data corresponding to the specified field.\n        \"\"\"\n        for subfield in field.split(\".\"):\n            if isinstance(input_doc, list):\n                input_doc = input_doc[0]\n            input_doc = input_doc.get(subfield)\n        return input_doc\n\n    column_names = [field.split(\".\")[-1] for field in fields]\n    return_df = pd.DataFrame(columns=column_names)\n\n    for document in documents:\n        temp_df = pd.DataFrame(\n            {column: get_field_data(document, field) for column, field in zip(column_names, fields)},\n            index=range(1)\n        )\n        return_df = pd.concat([return_df, temp_df]).reset_index(drop=True)\n\n    return return_df.fillna(value=np.nan)\n\ndef add_time_columns_to_dataframe(df):\n    \"\"\"\n    Adds additional time-related columns to a DataFrame based on the 'timestamp' column.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing a 'timestamp' column.\n\n    Returns:\n    pd.DataFrame: The DataFrame with added time-related columns.\n\n    Example:\n    import pandas as pd\n    df = pd.DataFrame({\n        'timestamp': pd.date_range(start='2022-01-01', end='2022-01-05', freq='D')\n    })\n    df = add_time_columns_to_dataframe(df)\n    print(df)\n    Output:\n       timestamp  Hours   Months  Days_Of_Month  Week_Days Month_Parts\n    0 2022-01-01      0  January              1   Saturday         1/5\n    1 2022-01-02      0  January              2     Sunday         1/5\n    2 2022-01-03      0  January              3     Monday         1/5\n    3 2022-01-04      0  January              4    Tuesday         2/5\n    4 2022-01-05      0  January              5  Wednesday         2/5\n    \"\"\"\n    # Add Hours column\n    df[\"Hours\"] = df[\"timestamp\"].dt.hour\n\n    # Add Months column\n    df[\"Months\"] = df[\"timestamp\"].dt.month_name()\n\n    # Add Days_Of_Month column\n    df[\"Days_Of_Month\"] = df[\"timestamp\"].dt.day\n\n    # Add Week_Days column\n    df[\"Week_Days\"] = df[\"timestamp\"].dt.day_name()\n\n    # Add Month_Parts column\n    df[\"Month_Parts\"] = df[\"Days_Of_Month\"] / 6\n    # Map Month_Parts to respective values\n    conditions = [\n        (df[\"Month_Parts\"] <= 1),\n        (df[\"Month_Parts\"] > 1) & (df[\"Month_Parts\"] <= 2),\n        (df[\"Month_Parts\"] > 2) & (df[\"Month_Parts\"] <= 3),\n        (df[\"Month_Parts\"] > 3) & (df[\"Month_Parts\"] <= 4),\n        (df[\"Month_Parts\"] > 4)\n    ]\n    values = [\"1/5\", \"2/5\", \"3/5\", \"4/5\", \"5/5\"]\n    df[\"Month_Parts\"] = np.select(conditions, values)\n    \n    return df\n\ndef add_expected_rows_to_count_analysis(df, category):\n    \"\"\"\n    Adds expected rows with zero count to a DataFrame based on a specified category.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing count analysis.\n    category (str): The category column to check and add missing rows.\n\n    Returns:\n    pd.DataFrame: The DataFrame with added expected rows.\n    \"\"\"\n    expected_row_values = CategoryToExpectedRowValuesMapping.get(category)\n    missing_rows = []\n    for value in expected_row_values:\n        if value not in df[category].values:\n            missing_rows.append(pd.DataFrame({category: [value], \"Count\": [0]}))\n    \n    if missing_rows:\n        df = pd.concat([df] + missing_rows, ignore_index=True)\n\n    # Sort the DataFrame based on the category to maintain order\n    df[category] = pd.Categorical(df[category], categories=expected_row_values, ordered=True)\n    df = df.sort_values(category).reset_index(drop=True)\n    \n    return df\n\n\ndef detect_outliers_iqr(data, k=1.5):\n    \"\"\"\n    Detect outliers in the data using IQR (Interquartile Range) method.\n\n    Parameters:\n    data (numpy.ndarray): Input data as a numpy array.\n    k (float): Coefficient to scale the IQR. Data points beyond k times the IQR\n               from the first and third quartiles are considered outliers. Default is 1.5.\n\n    Returns:\n    numpy.ndarray: Boolean array indicating whether each data point is an outlier or not.\n    \"\"\"\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n    lower_bound = q1 - k * iqr\n    upper_bound = q3 + k * iqr\n    return (data < lower_bound) | (data > upper_bound)"},"cell_type":"code","id":"ca58e3ff-72f9-4e73-a5ce-42a560fa8743","outputs":[],"execution_count":110},{"source":"# TESTING DOCUMENTS\n\nblood_pressure_documents = load_file_as_list_of_documents(filepath=\"data/BloodPressure.json\")\nblood_pressure_higher_mondays_documents = load_file_as_list_of_documents(filepath=\"data/blood_pressure_diastolic_higher_on_monday.json\")\ndiary_events_headache_on_monday = load_file_as_list_of_documents(filepath=\"data/diary_events_monday_heavy.json\")","metadata":{"executionCancelledAt":null,"executionTime":25,"lastExecutedAt":1712652179346,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# TESTING DOCUMENTS\n\nblood_pressure_documents = load_file_as_list_of_documents(filepath=\"data/BloodPressure.json\")\nblood_pressure_higher_mondays_documents = load_file_as_list_of_documents(filepath=\"data/blood_pressure_diastolic_higher_on_monday.json\")\ndiary_events_headache_on_monday = load_file_as_list_of_documents(filepath=\"data/diary_events_monday_heavy.json\")","lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec"},"cell_type":"code","id":"bb12d9b4-4edc-4496-8590-7ee19b5f1dba","outputs":[],"execution_count":92},{"source":"def execute(documents: list, values_aggregators: list[tuple]):\n    \"\"\"\n    Execute data analysis on a list of documents based on provided value aggregators.\n\n    Args:\n    documents (list): A list of dictionaries representing documents with nested fields.\n    values_aggregators (list): A list of tuples containing the value field and its aggregator function.\n\n    Returns:\n    dict: A dictionary containing the final result of the data analysis.\n    \"\"\"\n    value_fields = [value[0] for value in values_aggregators]\n    df = get_documents_data_into_df(documents=documents, fields=[\"timestamp\"] + value_fields)\n    df[\"timestamp\"] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_localize(None)\n    df = add_time_columns_to_dataframe(df=df)\n    \n    final_result_dict = {}\n    for value_field, aggregator in values_aggregators:\n        results_list = []\n        for category in group_by_categories:\n            result_dict = {}\n            temp_df = df[[category]]\n            if aggregator == \"size\":\n                temp_df = temp_df.groupby(category).size().fillna(0).reset_index(name=value_field)\n                temp_df = add_expected_rows_to_count_analysis(df=temp_df, category=category)\n            elif aggregator == \"mean\":\n                temp_df = df[[category, value_field]].groupby(category).mean().reset_index().round(2)\n            \n            outlier_boolean_array = detect_outliers_iqr(temp_df[value_field].values)\n\n            result_dict[\"category\"] = category\n            result_dict[\"series\"] = temp_df[value_field].values\n            result_dict[\"data_series_mean\"] = temp_df[value_field].mean() # Do we even want to include these in the result?\n            result_dict[\"data_series_min\"] = temp_df[value_field].min() # Do we even want to include these in the result?\n            result_dict[\"data_series_max\"] = temp_df[value_field].max() # Do we even want to include these in the result?\n            result_dict[\"outliers_values\"] = temp_df[value_field][outlier_boolean_array].values.tolist()\n            result_dict[\"outliers_column\"] = temp_df[temp_df[value_field].isin(result_dict[\"outliers_values\"])][category].values.tolist()\n            result_dict[\"outliers_column_values\"] = [temp_tuple for temp_tuple in temp_df[temp_df[value_field].isin(result_dict[\"outliers_values\"])][[category, value_field]].itertuples(index=False, name=None)] # Creates tuples of column and value pairs (\"January\", 9)\n\n            results_list.append(result_dict)\n\n        final_result_dict[value_field] = results_list\n    \n    return final_result_dict","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1712652866289,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def execute(documents: list, values_aggregators: list[tuple]):\n    \"\"\"\n    Execute data analysis on a list of documents based on provided value aggregators.\n\n    Args:\n    documents (list): A list of dictionaries representing documents with nested fields.\n    values_aggregators (list): A list of tuples containing the value field and its aggregator function.\n\n    Returns:\n    dict: A dictionary containing the final result of the data analysis.\n    \"\"\"\n    value_fields = [value[0] for value in values_aggregators]\n    df = get_documents_data_into_df(documents=documents, fields=[\"timestamp\"] + value_fields)\n    df[\"timestamp\"] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_localize(None)\n    df = add_time_columns_to_dataframe(df=df)\n    \n    final_result_dict = {}\n    for value_field, aggregator in values_aggregators:\n        results_list = []\n        for category in group_by_categories:\n            result_dict = {}\n            temp_df = df[[category]]\n            if aggregator == \"size\":\n                temp_df = temp_df.groupby(category).size().fillna(0).reset_index(name=value_field)\n                temp_df = add_expected_rows_to_count_analysis(df=temp_df, category=category)\n            elif aggregator == \"mean\":\n                temp_df = df[[category, value_field]].groupby(category).mean().reset_index().round(2)\n            \n            outlier_boolean_array = detect_outliers_iqr(temp_df[value_field].values)\n\n            result_dict[\"category\"] = category\n            result_dict[\"series\"] = temp_df[value_field].values\n            result_dict[\"data_series_mean\"] = temp_df[value_field].mean() # Do we even want to include these in the result?\n            result_dict[\"data_series_min\"] = temp_df[value_field].min() # Do we even want to include these in the result?\n            result_dict[\"data_series_max\"] = temp_df[value_field].max() # Do we even want to include these in the result?\n            result_dict[\"outliers_values\"] = temp_df[value_field][outlier_boolean_array].values.tolist()\n            result_dict[\"outliers_column\"] = temp_df[temp_df[value_field].isin(result_dict[\"outliers_values\"])][category].values.tolist()\n            result_dict[\"outliers_column_values\"] = [temp_tuple for temp_tuple in temp_df[temp_df[value_field].isin(result_dict[\"outliers_values\"])][[category, value_field]].itertuples(index=False, name=None)] # Creates tuples of column and value pairs (\"January\", 9)\n\n            results_list.append(result_dict)\n\n        final_result_dict[value_field] = results_list\n    \n    return final_result_dict","lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","outputsMetadata":{"0":{"height":164,"type":"stream"}}},"cell_type":"code","id":"3ff9ffe8-8a45-401b-894c-629342baa1c4","outputs":[],"execution_count":105},{"source":"result_headache = execute(documents=diary_events_headache_on_monday, values_aggregators=[(\"Count\", \"size\")])\nresult_bp = execute(documents=blood_pressure_higher_mondays_documents, values_aggregators=[(\"Count\", \"size\"), (\"diastolic_blood_pressure\", \"mean\")])","metadata":{"executionCancelledAt":null,"executionTime":114,"lastExecutedAt":1712652867065,"lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result_headache = execute(documents=diary_events_headache_on_monday, values_aggregators=[(\"Count\", \"size\")])\nresult_bp = execute(documents=blood_pressure_higher_mondays_documents, values_aggregators=[(\"Count\", \"size\"), (\"diastolic_blood_pressure\", \"mean\")])"},"cell_type":"code","id":"f896993b-a57d-40f6-9260-19691d597024","outputs":[],"execution_count":106},{"source":"result_headache[\"Count\"]","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1712652867797,"lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result_headache[\"Count\"]"},"cell_type":"code","id":"8574e4e1-590d-4baa-b52b-574b953c8381","outputs":[{"output_type":"execute_result","data":{"text/plain":"[{'category': 'Hours',\n  'series': array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         46]),\n  'data_series_mean': 2.5555555555555554,\n  'data_series_min': 0,\n  'data_series_max': 46,\n  'outliers_values': [46],\n  'outliers_column': [nan],\n  'outliers_column_values': [(nan, 46)]},\n {'category': 'Week_Days',\n  'series': array([23,  1,  8,  3,  5,  1,  5]),\n  'data_series_mean': 6.571428571428571,\n  'data_series_min': 1,\n  'data_series_max': 23,\n  'outliers_values': [23],\n  'outliers_column': ['Monday'],\n  'outliers_column_values': [('Monday', 23)]},\n {'category': 'Days_Of_Month',\n  'series': array([1, 1, 0, 2, 2, 3, 1, 3, 2, 2, 2, 3, 2, 0, 1, 1, 0, 2, 0, 2, 1, 1,\n         2, 1, 2, 1, 4, 1, 1, 1, 1]),\n  'data_series_mean': 1.4838709677419355,\n  'data_series_min': 0,\n  'data_series_max': 4,\n  'outliers_values': [4],\n  'outliers_column': [27],\n  'outliers_column_values': [(27, 4)]},\n {'category': 'Month_Parts',\n  'series': array([ 9, 13,  6,  7, 11]),\n  'data_series_mean': 9.2,\n  'data_series_min': 6,\n  'data_series_max': 13,\n  'outliers_values': [],\n  'outliers_column': [],\n  'outliers_column_values': []},\n {'category': 'Months',\n  'series': array([ 9,  2,  0,  0,  0,  0,  0,  0,  7,  9,  8, 11]),\n  'data_series_mean': 3.8333333333333335,\n  'data_series_min': 0,\n  'data_series_max': 11,\n  'outliers_values': [],\n  'outliers_column': [],\n  'outliers_column_values': []}]"},"metadata":{},"execution_count":107}],"execution_count":107},{"source":"result_bp[\"Count\"]","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1712652868588,"lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result_bp[\"Count\"]"},"cell_type":"code","id":"c16abab9-499d-4741-8609-e6933396cb8a","outputs":[{"output_type":"execute_result","data":{"text/plain":"[{'category': 'Hours',\n  'series': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1,\n         2, 1, 3, 2, 1, 1, 3, 1, 1, 1, 2]),\n  'data_series_mean': 0.7575757575757576,\n  'data_series_min': 0,\n  'data_series_max': 3,\n  'outliers_values': [3, 3],\n  'outliers_column': [nan, nan],\n  'outliers_column_values': [(nan, 3), (nan, 3)]},\n {'category': 'Week_Days',\n  'series': array([5, 7, 3, 2, 3, 3, 2]),\n  'data_series_mean': 3.5714285714285716,\n  'data_series_min': 2,\n  'data_series_max': 7,\n  'outliers_values': [7],\n  'outliers_column': ['Tuesday'],\n  'outliers_column_values': [('Tuesday', 7)]},\n {'category': 'Days_Of_Month',\n  'series': array([1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 2, 3, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0,\n         1, 0, 0, 0, 0, 0, 1, 1, 0]),\n  'data_series_mean': 0.8064516129032258,\n  'data_series_min': 0,\n  'data_series_max': 3,\n  'outliers_values': [3],\n  'outliers_column': [12],\n  'outliers_column_values': [(12, 3)]},\n {'category': 'Month_Parts',\n  'series': array([4, 8, 6, 5, 2]),\n  'data_series_mean': 5.0,\n  'data_series_min': 2,\n  'data_series_max': 8,\n  'outliers_values': [],\n  'outliers_column': [],\n  'outliers_column_values': []},\n {'category': 'Months',\n  'series': array([11,  6,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n  'data_series_mean': 2.0833333333333335,\n  'data_series_min': 0,\n  'data_series_max': 11,\n  'outliers_values': [11, 6, 8],\n  'outliers_column': ['January', 'February', 'March'],\n  'outliers_column_values': [('January', 11), ('February', 6), ('March', 8)]}]"},"metadata":{},"execution_count":108}],"execution_count":108},{"source":"result_bp[\"diastolic_blood_pressure\"]","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1712652869156,"lastExecutedByKernel":"780055f3-dc12-4f0d-b2a1-1f2b7fa18dec","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result_bp[\"diastolic_blood_pressure\"]"},"cell_type":"code","id":"bdce5d50-bed7-4822-9993-0047dc7252b2","outputs":[{"output_type":"execute_result","data":{"text/plain":"[{'category': 'Hours',\n  'series': array([60.  , 54.5 , 54.  , 65.  , 56.  , 55.5 , 54.  , 53.67, 55.5 ,\n         56.  , 61.  , 57.  , 58.  , 54.  , 60.  , 58.5 ]),\n  'data_series_mean': 57.041875000000005,\n  'data_series_min': 53.67,\n  'data_series_max': 65.0,\n  'outliers_values': [],\n  'outliers_column': [],\n  'outliers_column_values': []},\n {'category': 'Week_Days',\n  'series': array([55.67, 62.2 , 55.33, 58.  , 55.  , 54.57, 57.  ]),\n  'data_series_mean': 56.824285714285715,\n  'data_series_min': 54.57,\n  'data_series_max': 62.2,\n  'outliers_values': [62.2],\n  'outliers_column': ['Monday'],\n  'outliers_column_values': [('Monday', 62.2)]},\n {'category': 'Days_Of_Month',\n  'series': array([61.  , 56.  , 56.  , 58.  , 60.  , 55.5 , 55.  , 56.67, 54.  ,\n         56.  , 58.  , 57.  , 60.  , 54.5 , 58.  , 53.  , 70.  , 54.  ]),\n  'data_series_mean': 57.37055555555556,\n  'data_series_min': 53.0,\n  'data_series_max': 70.0,\n  'outliers_values': [70.0],\n  'outliers_column': [29],\n  'outliers_column_values': [(29, 70.0)]},\n {'category': 'Month_Parts',\n  'series': array([57.75, 56.38, 56.5 , 55.6 , 62.  ]),\n  'data_series_mean': 57.646,\n  'data_series_min': 55.6,\n  'data_series_max': 62.0,\n  'outliers_values': [62.0],\n  'outliers_column': ['5/5'],\n  'outliers_column_values': [('5/5', 62.0)]},\n {'category': 'Months',\n  'series': array([55.33, 56.82, 58.25]),\n  'data_series_mean': 56.800000000000004,\n  'data_series_min': 55.33,\n  'data_series_max': 58.25,\n  'outliers_values': [],\n  'outliers_column': [],\n  'outliers_column_values': []}]"},"metadata":{},"execution_count":109}],"execution_count":109}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}